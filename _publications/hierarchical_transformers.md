---
title: "Hierarchical Transformers Are More Efficient Language Models (NAACL 2022)"
collection: publications
category: manuscripts
permalink: /publication/2022-01-01-hierarchical-transformers
excerpt: 'Co-authored with Google Research. Improving efficiency in managing long sequences within the attention mechanism.'
date: 2022-07-01
venue: 'NAACL 2022'
slidesurl: ''
paperurl: 'https://arxiv.org/abs/2110.13711'
citation: 'Czechowski, K., Odrzygóźdź, T., Zbysiński, M., Zawalski, M., Olejnik, K., Wu, Y., Kuciński, Ł., Tyrolski, M., and Miłoś, P. (2022). "Hierarchical Transformers Are More Efficient Language Models." NAACL 2022.'
image: '' # Paste image path here, e.g. /images/publications/hierarchical_transformers.png
---

{% if page.image %}
<img src="{{ page.image }}" alt="Hierarchical Transformers Are More Efficient Language Models" style="max-width: 420px; border-radius: 8px; margin-bottom: 1em;" />
{% endif %}

Co-authored a paper with Google Research. Improving efficiency in managing long sequences within the attention mechanism by introducing hierarchical transformer architecture.

<!-- Add more details or links as available. -->
