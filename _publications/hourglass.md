---
title: "Hierarchical Transformers Are More Efficient Language Models (NAACL 2022)"
collection: publications
category: manuscripts
permalink: /publication/2022-01-01-hierarchical-transformers
excerpt: 'Improving efficiency in managing long sequences within the attention mechanism.'
date: 2022-01-01
venue: 'NAACL 2022'
slidesurl: ''
paperurl: 'https://arxiv.org/abs/2110.13711'
citation: 'Your Name, You. (2022). &quot;Hierarchical Transformers Are More Efficient Language Models.&quot; <i>NAACL 2022</i>.'
---

Co-authored a paper with Google Research [ArXiv link](https://arxiv.org/abs/2110.13711).
Improving efficiency in managing long sequences within the attention mechanism.